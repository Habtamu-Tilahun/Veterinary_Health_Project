{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from IPython.display import display, Image\n",
    "from IPython import display\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train model=yolov8m.pt pretrained=True data=Annotations/data.yaml epochs=25 lr0=0.01 imgsz=640 plots=True project=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.26 🚀 Python-3.10.9 torch-1.13.1+cu117 CUDA:0 (NVIDIA A40, 45413MiB)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/habtamu/Substrate/YOLOv8/Annotations/val/labels.cache... 101\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/habtamu/Substrate/YOLOv8/Annotations/val/images/012946_I0000055_b.png: ignoring corrupt image/label: negative label values [   -0.83124    -0.58129]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/habtamu/Substrate/YOLOv8/Annotations/val/images/012996_I0000604_b.png: ignoring corrupt image/label: negative label values [   -0.70685    -0.95099]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/habtamu/Substrate/YOLOv8/Annotations/val/images/013206_I0000982_b.png: ignoring corrupt image/label: negative label values [   -0.99967    -0.99967]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/habtamu/Substrate/YOLOv8/Annotations/val/images/013765_I0000189_b.png: ignoring corrupt image/label: negative label values [   -0.38078    -0.68132]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /home/habtamu/Substrate/YOLOv8/Annotations/val/images/013765_I0000191_b.png: ignoring corrupt image/label: negative label values [    -0.3976    -0.98532]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all         96         96      0.999          1      0.995      0.995\n",
      "Speed: 2.5ms preprocess, 7.8ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/val\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=val model=/home/habtamu/Substrate/YOLOv8/runs/train/weights/best.pt data=Annotations/data.yaml project=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo task=detect mode=predict model=/home/habtamu/Substrate/YOLOv8/runs/train/weights/best.pt conf=0.75 source=Annotations/test/images save=True project=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get bbox coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/train/weights/best.pt')\n",
    "validation_results = model.predict(source='Annotations/val/images', conf=0.67, project='runs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "tensor([[ 453.3691,  471.5865, 2640.4343, 2493.6985]], device='cuda:0')\n",
      "tensor([0.9740], device='cuda:0')\n",
      "tensor([0.], device='cuda:0')\n",
      "[[     453.37      471.59      2640.4      2493.7]]\n",
      "[     453.37      471.59      2640.4      2493.7]\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_results))\n",
    "print(validation_results[0].boxes.xyxy)\n",
    "print(validation_results[0].boxes.conf)\n",
    "print(validation_results[0].boxes.cls)\n",
    "tensor_result = validation_results[0].boxes.xyxy\n",
    "numpy_array_result = tensor_result.cpu().numpy()\n",
    "print(numpy_array_result)\n",
    "flattened_array = numpy_array_result.flatten()\n",
    "print(flattened_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop images based on the predicted bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('runs/train/weights/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images(original_images_folder, cropped_images_folder, results):\n",
    "    i = 0\n",
    "    # Get a list of filenames in the folder and sort them\n",
    "    filenames = sorted(os.listdir(original_images_folder))\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.png'):\n",
    "            #print(\"i=\", i)\n",
    "            input_file_path = os.path.join(original_images_folder, filename)\n",
    "            output_file_path = os.path.join(cropped_images_folder,filename)\n",
    "            img = cv2.imread(input_file_path)\n",
    "            bbox_tensor_result = results[i].boxes.xyxy\n",
    "            if bbox_tensor_result.shape[0] >= 2:\n",
    "                bbox_numpy_result = bbox_tensor_result[0].cpu().numpy().flatten()\n",
    "            else:\n",
    "                bbox_numpy_result = bbox_tensor_result.cpu().numpy().flatten()\n",
    "            if bbox_numpy_result.shape[0]!=0:\n",
    "                x_min, y_min, x_max, y_max = bbox_numpy_result\n",
    "            else:\n",
    "                # Get image dimensions\n",
    "                height, width, _ = img.shape\n",
    "                x_min, y_min, x_max, y_max = 0, 0, width, height\n",
    "            \n",
    "            #img_copy = img.copy()\n",
    "            # display the image with its roi bbox\n",
    "            #cv2.rectangle(img_copy, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (255, 0, 0), 8)\n",
    "            #plt.imshow(img_copy)\n",
    "            #plt.show()\n",
    "        \n",
    "            # Crop and save the image\n",
    "            img = img[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "            cv2.imwrite(output_file_path,img)\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop validation images\n",
    "original_images_folder = \"Annotations/val/images\"\n",
    "cropped_images_folder = \"Cropped_val_images\"\n",
    "crop_images(original_images_folder, cropped_images_folder,validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop OK_bluecare_feb24 images\n",
    "ok_bluecare_feb24_results = model.predict(source='/home/habtamu/data/substrate/OK_bluecare_feb24/prova', conf=0.67, project='runs')\n",
    "original_images_folder = \"/home/habtamu/data/substrate/OK_bluecare_feb24/prova\"\n",
    "cropped_images_folder = \"/home/habtamu/data/substrate/OK_bluecare_feb24_cropped\"\n",
    "crop_images(original_images_folder, cropped_images_folder,ok_bluecare_feb24_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop OK_cullera_feb24 images\n",
    "ok_cullera_feb24_results = model.predict(source='/home/habtamu/data/substrate/OK_cullera_feb24/prova', conf=0.67, project='runs')\n",
    "original_images_folder = \"/home/habtamu/data/substrate/OK_cullera_feb24/prova\"\n",
    "cropped_images_folder = \"/home/habtamu/data/substrate/OK_cullera_feb24_cropped\"\n",
    "crop_images(original_images_folder, cropped_images_folder,ok_cullera_feb24_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop KO_bluecare images\n",
    "ko_bluecare_results = model.predict(source='/home/habtamu/data/substrate/KO_bluecare/prova', conf=0.67, project='runs')\n",
    "original_images_folder = \"/home/habtamu/data/substrate/KO_bluecare/prova\"\n",
    "cropped_images_folder = \"/home/habtamu/data/substrate/KO_bluecare_cropped\"\n",
    "crop_images(original_images_folder, cropped_images_folder,ko_bluecare_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop KO_cullera images\n",
    "ko_cullera_results = model.predict(source='/home/habtamu/data/substrate/KO_cullera/prova', conf=0.67, project='runs')\n",
    "original_images_folder = \"/home/habtamu/data/substrate/KO_cullera/prova\"\n",
    "cropped_images_folder = \"/home/habtamu/data/substrate/KO_cullera_cropped\"\n",
    "crop_images(original_images_folder, cropped_images_folder, ko_cullera_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habtamukernel",
   "language": "python",
   "name": "habtamukernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
